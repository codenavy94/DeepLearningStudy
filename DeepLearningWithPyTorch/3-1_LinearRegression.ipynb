{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1],\n",
    "                             [2],\n",
    "                             [3]])\n",
    "y_train = torch.FloatTensor([[2],\n",
    "                             [4],\n",
    "                             [6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형회귀: 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일\n",
    "- 선형회귀의 가설(직선의 방정식)\n",
    "\n",
    "$ y = Wx + b $  \n",
    "$ H(x) = Wx + b $\n",
    "\n",
    "- $W$: 가중치(weight)\n",
    "- $b$: 편향(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 비용 함수(Cost function)에 대한 이해\n",
    "\n",
    "- 비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)\n",
    "\n",
    "- 평균제곱오차(Mean Sqared Error)\n",
    "\n",
    "$ cost(W, b) = \\frac{1}{n} \\sum\\limits_{i=1}^n [y^i - H(x^i)]^2 $\n",
    "\n",
    "- 평균제곱오차(즉, 비용함수)의 값을 \"최소값\"으로 만드는 $W$와 $b$를 찾아내는 것이 학습의 목적!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 옵티마이저 - 경사 하강법(Gradient Descent)\n",
    "\n",
    "- 옵티마이저(optimizer) 알고리즘 = 최적화 알고리즘\n",
    "- 옵티마이저 알고리즘을 통해 적절한 $W$와 $b$를 찾아내는 과정을 머신러닝에서 학습(training)이라고 부른다.\n",
    "- 가장 기본적인 옵티마이저 알고리즘: Gradient Descent\n",
    "\n",
    "#### Gradient Descent\n",
    "- cost가 최소화되는 지점은 접선의 기울기가 0이 되는 지점, 즉, 미분값이 0이 되는 지점\n",
    "- 경사하강법의 아이디어: 비용함수를 미분하여 현재 W에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 W의 값을 변경하는 작업을 반복하는 것!\n",
    "\n",
    "- 기울기 = $ \\frac{d cost(W)}{d W} $\n",
    "\n",
    "- 새로운 기울기 = $ W - \\alpha \\cdot \\frac{d cost(W)}{d W} $\n",
    "\n",
    "**기울기가 음수일 때**\n",
    "\n",
    "$ W - \\alpha \\cdot$(음수기울기) $= W + \\alpha \\cdot$(양수기울기)  \n",
    "(즉, W의 값이 증가하게 된다)\n",
    "\n",
    "**기울기가 양수일 때**\n",
    "\n",
    "$ W - \\alpha \\cdot$(양수기울기) $  \n",
    "(즉, W의 값이 감소하게 된다)\n",
    "\n",
    "- $\\alpha$(learning rate): W의 값을 얼마나 크게 변경할지 그 폭을 결정하는 값.  \n",
    "지나치게 크면 발산, 작으면 수렴하지 못하는 문제가 발생할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PyTorch로 선형회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c2d6944bd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 선언\n",
    "x_train = torch.FloatTensor([[1],\n",
    "                             [2],\n",
    "                             [3]])\n",
    "y_train = torch.FloatTensor([[2],\n",
    "                             [4],\n",
    "                             [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치와 편향의 초기화\n",
    "# 가중치를 0으로 초기화하고, 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "W = torch.zeros(1, requires_grad=True) # requires_grad 파라미터는 '학습을 통해 값이 계속 변경된다'는 것을 의미함!\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 가설 세우기\n",
    "hypothesis = x_train * W + b # (3, 1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 비용 함수 선언하기\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법 구현하기\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "optimizer.zero_grad() # gradient를 0으로 초기화\n",
    "cost.backward() # 비용함수를 미분하여 gradient 계산\n",
    "optimizer.step() # W와 b를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.18666668236255646, b: 0.07999999821186066, Cost: 18.66666603088379\n",
      "Epoch  100/2000 W: 1.7456912994384766, b: 0.5780722498893738, Cost: 0.04817060008645058\n",
      "Epoch  200/2000 W: 1.80009925365448, b: 0.4544214904308319, Cost: 0.029766537249088287\n",
      "Epoch  300/2000 W: 1.8428597450256348, b: 0.35721680521965027, Cost: 0.018393920734524727\n",
      "Epoch  400/2000 W: 1.876473307609558, b: 0.28080499172210693, Cost: 0.011366334743797779\n",
      "Epoch  500/2000 W: 1.9028971195220947, b: 0.22073803842067719, Cost: 0.007023668382316828\n",
      "Epoch  600/2000 W: 1.9236682653427124, b: 0.1735200434923172, Cost: 0.00434019835665822\n",
      "Epoch  700/2000 W: 1.9399964809417725, b: 0.13640251755714417, Cost: 0.0026819754857569933\n",
      "Epoch  800/2000 W: 1.952831506729126, b: 0.10722480714321136, Cost: 0.0016572937602177262\n",
      "Epoch  900/2000 W: 1.9629212617874146, b: 0.0842885971069336, Cost: 0.0010241125710308552\n",
      "Epoch 1000/2000 W: 1.9708527326583862, b: 0.06625857204198837, Cost: 0.0006328357267193496\n",
      "Epoch 1100/2000 W: 1.977087378501892, b: 0.05208539217710495, Cost: 0.0003910581872332841\n",
      "Epoch 1200/2000 W: 1.9819889068603516, b: 0.04094360023736954, Cost: 0.0002416476490907371\n",
      "Epoch 1300/2000 W: 1.9858416318893433, b: 0.032185304909944534, Cost: 0.00014932315389160067\n",
      "Epoch 1400/2000 W: 1.9888702630996704, b: 0.025300635024905205, Cost: 9.227206464856863e-05\n",
      "Epoch 1500/2000 W: 1.991250991821289, b: 0.019888687878847122, Cost: 5.702077760361135e-05\n",
      "Epoch 1600/2000 W: 1.9931223392486572, b: 0.015634294599294662, Cost: 3.523503983160481e-05\n",
      "Epoch 1700/2000 W: 1.994593620300293, b: 0.01229004468768835, Cost: 2.1773657863377593e-05\n",
      "Epoch 1800/2000 W: 1.995750069618225, b: 0.009661080315709114, Cost: 1.3454421605274547e-05\n",
      "Epoch 1900/2000 W: 1.9966591596603394, b: 0.007594636641442776, Cost: 8.314520528074354e-06\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1],\n",
    "                             [2],\n",
    "                             [3]])\n",
    "y_train = torch.FloatTensor([[2],\n",
    "                             [4],\n",
    "                             [6]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2000 # 원하는 만큼 경사하강법을 반복\n",
    "for epoch in range(nb_epochs):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # gradient 초기화\n",
    "    cost.backward() # gradient(미분값) 계산\n",
    "    optimizer.step() # [W, b] update\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d}/{nb_epochs} W: {W.item()}, b: {b.item()}, Cost: {cost.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- epoch: 전체 훈련 데이터가 학습에 한 번 사용된 주기(이번의 경우 2,000번 수행)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb98f1078c0bf0908da4d541c97382695dcf7ee9c0c20aad61f626557213d022"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('workspace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
