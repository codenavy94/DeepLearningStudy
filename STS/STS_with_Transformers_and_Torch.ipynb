{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Codes originally from James Briggs and his YouTube Video on ['Sentence Similarity With Transformers and PyTorch'](https://www.youtube.com/watch?v=jVPd7lEvjtg)\n",
    "\n",
    "- The codes have been rewritten by codenavy94 for studying purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"Standing on one's head at job interviews forms a lasting impression.\",\n",
    "    \"It took him a month to finish the meal.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]\n",
    "\n",
    "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 399/399 [00:00<00:00, 133kB/s]\n",
      "Downloading: 100%|██████████| 625/625 [00:00<00:00, 208kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 275kB/s]  \n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 484kB/s]  \n",
      "Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 1.00kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 37.9kB/s]\n",
      "Downloading: 100%|██████████| 418M/418M [00:13<00:00, 33.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {'input_ids':[], 'attention_mask':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    new_tokens = tokenizer.encode_plus(sentence,\n",
    "                                       max_length=128, # how many tokens per each sequence (sentence)\n",
    "                                       truncation=True,\n",
    "                                       padding='max_length',\n",
    "                                       return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'] = torch.stack(tokens['input_ids']) # a list of tensors to a single tensor (with an extra dimension)\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape # sentence_num, token_num(max_length) # Warning: token_num != embedding_size (768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings.shape # 6 sentences, 128 tokens, 768 hidden_state size of BERT\n",
    "# each token is represented as a (768,) size of hidden state vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove embedding values from where there shouldn't be embedding values\n",
    "# Padding tokens, for example, aren't real tokens.\n",
    "# Therefore, we remove the embedding values from such 'fake' tokens, since you don't want your model to focus on the padding tokens!\n",
    "attention = tokens['attention_mask']\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = attention.unsqueeze(-1).expand(embeddings.shape).float() # add a dimension and expand it to match the size of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_embeddings = embeddings * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0692,  0.6230,  0.0354,  ...,  0.8033,  1.6314,  0.3281],\n",
       "         [ 0.0367,  0.6842,  0.1946,  ...,  0.0848,  1.4747, -0.3008],\n",
       "         [-0.0121,  0.6543, -0.0727,  ..., -0.0326,  1.7717, -0.6812],\n",
       "         ...,\n",
       "         [ 0.6712,  1.1871,  0.2726,  ...,  0.9307,  0.9777, -0.2166],\n",
       "         [ 0.5063,  1.1734,  0.3907,  ...,  0.8818,  0.9201, -0.2802],\n",
       "         [ 0.3268,  1.0662,  0.2948,  ...,  0.9246,  1.0575, -0.3765]],\n",
       "\n",
       "        [[-0.3212,  0.8251,  1.0554,  ..., -0.1855,  0.1517,  0.3937],\n",
       "         [-0.7146,  1.0297,  1.1217,  ...,  0.0331,  0.2382, -0.1563],\n",
       "         [-0.2352,  1.1353,  0.8594,  ..., -0.4310, -0.0272, -0.2968],\n",
       "         ...,\n",
       "         [-0.0894,  0.5312,  1.1392,  ..., -0.2256, -0.5657,  0.1937],\n",
       "         [-0.2446,  0.5845,  1.1385,  ..., -0.2867, -0.5586, -0.1296],\n",
       "         [-0.1811,  0.6800,  1.2574,  ..., -0.2590, -0.5041, -0.0792]],\n",
       "\n",
       "        [[-0.7576,  0.8399, -0.3792,  ...,  0.1271,  1.2514,  0.1365],\n",
       "         [-0.6591,  0.7613, -0.4662,  ...,  0.2259,  1.1289, -0.3611],\n",
       "         [-0.9007,  0.6791, -0.3778,  ...,  0.1142,  0.9080, -0.1830],\n",
       "         ...,\n",
       "         [-0.1714,  0.8286,  0.1518,  ...,  0.2081,  0.1452, -0.2136],\n",
       "         [-0.0931,  0.9175,  0.0958,  ...,  0.1994,  0.1539, -0.4138],\n",
       "         [-0.2398,  0.9074, -0.0174,  ...,  0.3267,  0.3449, -0.4489]],\n",
       "\n",
       "        [[-0.1025,  0.9784,  1.4798,  ..., -0.6732, -1.3459, -0.1541],\n",
       "         [ 0.1646,  1.1261,  0.9745,  ..., -0.8240, -1.5562, -0.6040],\n",
       "         [ 0.4792,  0.9723,  1.3746,  ..., -0.9825, -1.3523, -0.5883],\n",
       "         ...,\n",
       "         [ 0.4943,  0.5731,  1.2877,  ..., -0.5721, -1.4697,  0.0862],\n",
       "         [ 0.4266,  0.4777,  1.3289,  ..., -0.4623, -1.4224, -0.0559],\n",
       "         [ 0.4440,  0.6825,  1.7366,  ..., -0.6220, -1.7459, -0.3368]],\n",
       "\n",
       "        [[-0.0694,  0.1394,  0.7976,  ...,  0.1190,  0.9882,  0.2658],\n",
       "         [ 0.0051, -0.0535,  0.8865,  ..., -0.2087,  0.7960,  0.0292],\n",
       "         [-0.1518,  0.0141,  0.7603,  ..., -0.2641,  0.6399, -0.1505],\n",
       "         ...,\n",
       "         [-0.1771, -0.0243,  0.7141,  ...,  0.2158,  0.4730, -0.0569],\n",
       "         [-0.0572,  0.0733,  0.7643,  ...,  0.3044,  0.4026, -0.1921],\n",
       "         [-0.0642,  0.2244,  0.8992,  ...,  0.4557,  0.5453, -0.3453]],\n",
       "\n",
       "        [[-0.2362,  0.8551, -0.8040,  ...,  0.6122,  0.3003, -0.1492],\n",
       "         [-0.0868,  0.9531, -0.6419,  ...,  0.7867,  0.2960, -0.7350],\n",
       "         [-0.3016,  1.0148, -0.3380,  ...,  0.8634,  0.0463, -0.3623],\n",
       "         ...,\n",
       "         [ 0.3023,  1.1593, -1.0741,  ...,  0.8951, -0.2590, -0.2359],\n",
       "         [ 0.3244,  0.9653, -1.0071,  ...,  0.8258, -0.2099, -0.3995],\n",
       "         [ 0.1305,  1.2129, -0.6874,  ...,  1.0314, -0.1676, -0.5557]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0692,  0.6230,  0.0354,  ...,  0.8033,  1.6314,  0.3281],\n",
       "         [ 0.0367,  0.6842,  0.1946,  ...,  0.0848,  1.4747, -0.3008],\n",
       "         [-0.0121,  0.6543, -0.0727,  ..., -0.0326,  1.7717, -0.6812],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.3212,  0.8251,  1.0554,  ..., -0.1855,  0.1517,  0.3937],\n",
       "         [-0.7146,  1.0297,  1.1217,  ...,  0.0331,  0.2382, -0.1563],\n",
       "         [-0.2352,  1.1353,  0.8594,  ..., -0.4310, -0.0272, -0.2968],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.7576,  0.8399, -0.3792,  ...,  0.1271,  1.2514,  0.1365],\n",
       "         [-0.6591,  0.7613, -0.4662,  ...,  0.2259,  1.1289, -0.3611],\n",
       "         [-0.9007,  0.6791, -0.3778,  ...,  0.1142,  0.9080, -0.1830],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.1025,  0.9784,  1.4798,  ..., -0.6732, -1.3459, -0.1541],\n",
       "         [ 0.1646,  1.1261,  0.9745,  ..., -0.8240, -1.5562, -0.6040],\n",
       "         [ 0.4792,  0.9723,  1.3746,  ..., -0.9825, -1.3523, -0.5883],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.0694,  0.1394,  0.7976,  ...,  0.1190,  0.9882,  0.2658],\n",
       "         [ 0.0051, -0.0535,  0.8865,  ..., -0.2087,  0.7960,  0.0292],\n",
       "         [-0.1518,  0.0141,  0.7603,  ..., -0.2641,  0.6399, -0.1505],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.2362,  0.8551, -0.8040,  ...,  0.6122,  0.3003, -0.1492],\n",
       "         [-0.0868,  0.9531, -0.6419,  ...,  0.7867,  0.2960, -0.7350],\n",
       "         [-0.3016,  1.0148, -0.3380,  ...,  0.8634,  0.0463, -0.3623],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_embeddings # Among 6 x 128 x 768 vectors, replace 768-sized vectors with 0 for tokens that are not real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Pooling Operation\n",
    "# Currently, each token has a separate latent vector of size (768,)\n",
    "# Getting a 'sentence embedding' requires mean pooling of 128 tokens (i.e., their hidden state embeddings)\n",
    "\n",
    "summed = torch.sum(mask_embeddings, dim=1) # mask_embeddings.shape => (6, 128, 768) after sum => (6, 768), each sentence is represented as a (768,) embedding\n",
    "summed.shape # the second dimension removed (summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled = summed / counts\n",
    "mean_pooled.shape # <= This is our sentence vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0745,  0.8637,  0.1795,  ...,  0.7734,  1.7247, -0.1803],\n",
       "        [-0.3715,  0.9729,  1.0840,  ..., -0.2552, -0.2759,  0.0358],\n",
       "        [-0.5030,  0.7950, -0.1240,  ...,  0.1441,  0.9704, -0.1791],\n",
       "        [-0.0132,  0.9773,  1.4516,  ..., -0.8462, -1.4004, -0.4118],\n",
       "        [-0.2019,  0.0597,  0.8603,  ..., -0.0100,  0.8431, -0.0841],\n",
       "        [-0.2131,  1.0175, -0.8833,  ...,  0.7371,  0.1947, -0.3011]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3308891 , 0.7219258 , 0.17475498, 0.4470964 , 0.55483633]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "cosine_similarity(\n",
    "    [mean_pooled[0]],\n",
    "    mean_pooled[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"Standing on one's head at job interviews forms a lasting impression.\",\n",
    "    \"It took him a month to finish the meal.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb98f1078c0bf0908da4d541c97382695dcf7ee9c0c20aad61f626557213d022"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('workspace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
